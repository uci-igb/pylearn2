!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {
        which_set: 'train',
        one_hot: 1,
        start: 0,
        stop: 50000 # 50000
    },
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [ !obj:pylearn2.models.mlp.Sigmoid {
                     layer_name: 'h0',
                     dim: 800,
                     sparse_init: 15,  # all in dropout paper.
                     sparse_stdev: 1., # ".01" in dropout paper.
                     W_lr_scale: .64,   # Should match dropout p^2. See costs/mlp/dropout.py
                     b_lr_scale: 1.,    # Doesn't change.
                 }, !obj:pylearn2.models.mlp.Sigmoid {
                     layer_name: 'h1',
                     dim: 800,
                     sparse_init: 15, # OR irange=1, s.t. initial weights U(-irange,irange)
                     sparse_stdev: 1.,
                     W_lr_scale: .25, # Should match dropout p^2 for this layer in cost.
                     b_lr_scale: 1.,    # Doesn't change.
                 }, !obj:pylearn2.models.mlp.Softmax {
                     layer_name: 'y',
                     n_classes: 10,
                     sparse_init: 15, 
                     #sparse_stdev: 1., # Doesn't matter for softmax.
                     W_lr_scale: .25,    # Should match dropout p^2 for this layer in cost.
                     b_lr_scale: 1.,     # Doesn't change.
                 }
                ],
        nvis: 784,
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 100,
        learning_rate: .1,
        init_momentum: .5,
        monitoring_dataset:
        {
                'train' : *train,
                'valid' : !obj:pylearn2.datasets.mnist.MNIST {
                              which_set: 'train',
                              one_hot: 1,
                              start: 50000,
                              stop:  60000
                          },
                'test'  : !obj:pylearn2.datasets.mnist.MNIST {
                              which_set: 'test',
                              one_hot: 1,
                          }
        },
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: "valid_y_misclass",
            prop_decrease: 0.,
            N: 10   # Number of iterations to look back.
        },
        #termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
        #        max_epochs: 3,
        #},
        cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
                input_include_probs: { 'h0' : .8 },
                input_scales: { 'h0': 1.25 }, # Crazy, should probably change.
                input_include_probs: {'h1' : .5},
                input_scales: { 'h1': 2.},
                input_include_probs: {'y' : .5},
                input_scales: { 'y': 2.},
        },
        update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
                decay_factor: 1.000004,
                min_lr: .000001
        }
    },
            
    extensions: [ !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "dropout.pkl"
        }, !obj:pylearn2.training_algorithms.sgd.MomentumAdjustor {
            start: 1,
            saturate: 10,
            final_momentum: .99
        }
    ]
}
